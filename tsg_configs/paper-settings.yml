# Job Settings
id: debug
gpu: 0

# Input Settings
checkpoint_path: tsgmt3
input_json: /home/henry/Datasets/coco/cocotalk.json
input_label_h5: /home/henry/Datasets/coco/cocotalk_label.h5
input_att_dir: /home/henry/Datasets/coco/butd_att/
input_rela_dir: /home/henry/Datasets/coco/coco_img_sg
cached_tokens: /home/henry/Datasets/coco/coco-train-idxs
# checkpoint_path: /data/scratch/eey362/tsgmt1
# input_json: /data/EECS-YuanLab/COCO/cocotalk.json
# input_label_h5: /data/EECS-YuanLab/COCO/cocotalk_label.h5
# input_att_dir: /data/EECS-YuanLab/COCO/butd_att/
# input_rela_dir: /data/EECS-YuanLab/COCO/coco_img_sg/
# cached_tokens: /data/EECS-YuanLab/COCO/coco-train-idxs


# Model Settings
caption_model: tsgm3
num_layers: 6
input_encoding_size: 512
rnn_size: 2048 #d_ff

# Transformer config
N_enc: 6
N_dec: 6
d_model: 512
d_ff: 2048
num_att_heads: 8
dropout: 0.1

# General Training Settings
max_epochs: 50
batch_size: 8
save_checkpoint_every: 30000
language_eval: 1
rp_decay_every: 3000 # iterations before LR decay
val_images_use: 5000
train_sample_n: 5

# XE Training Settings
learning_rate: 0.0005
learning_rate_decay_start: 0
learning_rate_decay_every: 5
learning_rate_decay_rate: 0.8
noamopt: false # NoamOpt is the Transformer lambda function optimiser
noamopt_warmup: 20000

# Structured Learning Settings
structure_after: -1
structure_loss_weight: 1
structure_loss_type: new_self_critical


# SCST/RL Learning Settings
self_critical_after: 20
noamopt_rl: false
learning_rate_rl: 0.00005
learning_rate_decay_start_rl: 0
learning_rate_decay_every_rl: 5
learning_rate_decay_rate_rl: 0.8
reduce_on_plateau_rl: true
train_sample_n_rl: 5

# Things set to something other than default but not used
concat_type: 1
controller: 1

